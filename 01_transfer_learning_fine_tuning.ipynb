{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true) <img src=\"images/practicumai_transfer_learning.png\" alt=\"Practicum AI: Transfer Learning icon\" align=\"right\" width=50>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Concepts\n",
    "\n",
    "You may recall *Practicum AI*'s heroine Amelia, the AI-savvy nutritionist. At the end of our *[Deep Learning Foundations course](https://practicumai.org/courses/deep_learning/)*, Amelia was helping with a computer vision project. If only she had known about transfer learning, it could have saved her a lot of time! In this notebook, we will get some hands-on experience with transfer learning and show you how to use it to improve your workflows.\n",
    "\n",
    "<img src='images/agrinet_figure-cover.jpg' alt=\"Figure 2 of the AgriNet paper used as the cover image for this notebook. Figure 2 depicts using transfer learning to make a computer vision model more efficient\" width=60%>\n",
    "\n",
    "## AgriNet background\n",
    "\n",
    "This exercise is based off the 2022 Sahili and Awad *Frontiers in Plant Sciences* paper [The power of transfer learning in agricultural applications: AgriNet](https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2022.992700/full). In this paper Sahili and Awad explore different models using a large dataset of over 160,000 images. While the full dataset is too large for this exercise, we have made a subset of the data, and will use a similar approach as we explore transfer learning.\n",
    "\n",
    "## AI Pathway review for transfer learning & AgriNet \n",
    "\n",
    "If you have taken our [Getting Started with AI course](https://practicumai.org/courses/getting_started/), you may remember this figure of the AI Application Development Pathway. Let's take a quick review of how we will apply this to our case study of AgriNet and it's use of transfer learning.\n",
    "\n",
    "![AI Application Development Pathway image showing the 7 steps in developing an AI application](https://practicumai.org/getting_started/images/application_dev_pathway.png)\n",
    "\n",
    "1. **Choose a problem to solve:** In this example, we will try to make a computer vision model that can recognize images of plants and categorize them as \"healthy\" or in one or more disease classes. As you will see when you explore the data, there are over 20 crops and some have as many as 11 disease states. In total, there are 73 categories!\n",
    "2. **Gather data:** The data for the example comes from [HuggingFace](https://huggingface.co/datasets/zahraa/AgriNet), a great repository of datasets, code, and models. \n",
    "3. **Clean and prepare the data:** While the dataset is already fairly good, we have done some further processing of the data:\n",
    "    * We have removed some crops with no \"healthy\" category. We have combined lemon into the Citrus category. We dropped some crops with relatively few images. \n",
    "    * Most importantly, we have made the dataset a bit more realistic and manageable for the course. **We have randomly subsampled the images so that each category has no more than 100 images in the training set**.\n",
    "    * The details of the modifications are in the [Data_processing.ipynb notebook](Data_processing.ipynb).\n",
    "4. **Choose a model:** For reasons of efficiency and training speed, we will use the EfficientNet-B5 model (more on that below). The EfficientNet models are Convolutional Neural Networks (CNNs). We will train from scratch as a baseline (using randomly initialized weights, not the pre-trained weights), and compare that to starting with a pre-trained model that we fine-tune on the AgriNet dataset. We will use an EfficientNet-B5 model trained on the ImageNet dataset, which is a large dataset of images with 1,000 classes. By fine-tuning this model, we can leverage the knowledge the model has learned from the ImageNet dataset and apply it to our specific problem of plant disease classification. Since the *domain* of ImageNet is distinctly different (1,000 everyday objects) from the domain of AgriNet (healthy and diseased plants), this is a **Domain Transfer** example. \n",
    "   * In the step where you'd choose a model, one can approach this many ways, for this notebook we'll just mention two:\n",
    "      * **Train from scratch:** This is where you start with a randomly initialized model and train it on your data. This can be computationally expensive and time-consuming.\n",
    "      * **Domain Transfer via Fine-Tuning:** This is where you start with a pre-trained model and fine-tune it on your data. This is often faster and requires less data.\n",
    "5. **Train the model:** As mentioned in step 4, we'll demonstrate two approaches in this notebook:\n",
    "      - Training the EfficientNet model architecture from scratch.\n",
    "      - Fine-tuning an EfficientNet model pre-trained on ImageNet, a domain-specific dataset to achieve Domain Transfer.\n",
    "6. **Evaluate the model:** We will use the metrics we gather to make decisions about the model. \n",
    "7. **Deploy the model:** We won't get to this stage in this course, but ideally we would end up with a model that could be deployed and achieve relatively good accuracy at solving crop classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Lightning \n",
    "\n",
    "<img src='images/Lightning_logo.svg' alt=\"PyTorch Lightning logo\" width=25%>\n",
    "\n",
    "[PyTorch Lightning](https://lightning.ai/) is an open-source framework built on top of PyTorch that makes training deep learning models more straightforward. It abstracts many common tasks like managing training loops, logging, checkpointing, and handling hardware setups, allowing you to focus on the core aspects of your model and experimentation. \n",
    "\n",
    "Rather than writing repetitive code, you define key methodsâ€”such as `training_step` and `validation_step`â€”to describe the model's behavior while the Lightning trainer automates optimization details, synchronization, and even distributed training. This separation between scientific code and engineering routines leads to cleaner, more maintainable projects that are easier to scale.\n",
    "\n",
    "Additionally, PyTorch Lightning integrates smoothly with popular tools such as [TensorBoard](https://www.tensorflow.org/tensorboard), which simplifies tracking experiment metrics and visualizing performance. Overall, Lightning streamlines the training workflow, boosts reproducibility, and helps both beginners and seasoned researchers concentrate on innovation, not boilerplate coding.\n",
    "\n",
    "This course will make use of Lightning to simplify training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries\n",
    "\n",
    "First, let's import the libraries we'll need. PyTorch is a popular open-source machine learning library for Python, and is developed by Meta's Facebook AI Research lab (FAIR). We'll also use PyTorch Lightning and some other commonly used supporting libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import ImageFile\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Import EfficientNet-B5\n",
    "from torchvision.models import efficientnet_b5\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for GPU availability\n",
    "\n",
    "This cell will check that everything is configured correctly to use your GPU. If everything is correct, you should see something like: \n",
    "\n",
    "    Using GPU: [type of GPU]\n",
    "\n",
    "If you see:\n",
    "    \n",
    "    Using CPU\n",
    "    \n",
    "Either you do not have a GPU or the kernel is not correctly configured to use it. You might be able to run this notebook, but some sections will take a loooooong time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(\"Setting Torch precision to medium for faster performance\")\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this notebook's `FIX_ME`s\n",
    "\n",
    "In this notebook, you'll find sections marked with:\n",
    "\n",
    "```\n",
    "# FIX_ME: <description of what to do>\n",
    "```\n",
    "These are places where you need to fill in missing code or make adjustments. The goal is to reinforce the key implementation steps for each technique.\n",
    "\n",
    "If you get stuck, review the preceding explanations, check the documentation for the libraries used (PyTorch, Hugging Face), or consult the course's **GitHub Discussions page** [Link to GitHub Discussions - Placeholder] for hints and help from peers and instructors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Load the data\n",
    "\n",
    "The dataset is located in the workshop project directory and is ready for you to use. The images are located in the `data` directory, with the training, validation and test sets loaded into `agri_net_train`, `agri_net_val` and `agri_net_test` respectively.\n",
    "\n",
    "If you'd later like to download your own copy of the data, we provide code for that, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to dataset on Atlas for USDA-ARS workshop\n",
    "data_path = Path('/project/scinet_workshop1/transfer_learning/data')\n",
    "\n",
    "# Uncomment if you want to install, or have installed, the data\n",
    "# in the current folder \n",
    "# data_path = \"data\"\n",
    "\n",
    "train_dir = data_path / 'agri_net_train100'\n",
    "val_dir = data_path / 'agri_net_val'\n",
    "test_dir = data_path / 'agri_net_test'\n",
    "\n",
    "# Run the lines below if you'd like to download the dataset yourself.\n",
    "#download_url = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning\"\n",
    "#file_name = \"agrinet_image_data.tar.gz\"\n",
    "#folder_names = [\"agri_net_train100\", \"agri_net_val\", \"agri_net_test\"]\n",
    "\n",
    "#helpers.download_and_extract_data(download_url, file_name, data_path, folder_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Look at the data\n",
    "We will take a look at the data to see what we are working with. This is a good practice to get a sense of the data and to see if there are any issues that need to be addressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = helpers.explore_data(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the Data Loaders\n",
    "\n",
    "Next, we'll create the data loaders for the training, validation and test sets. We'll apply some simple data augmentation to the training data. This is another thing that you could experiment with.\n",
    "\n",
    "While the GPU does most of the calculations in training AI models, the CPU of the computer server is responsible for loading images from disk, doing any transformations and sending the data to the GPU. PyTorch takes care of all of this and takes care of doing this in parallel. For maximum GPU performance, multiple cores are needed to constantly feed data to the GPU. The number of workers (`num_workers`) argument controls how many parallel tasks should be running to load data. \n",
    "\n",
    "The code block below will detect if your notebook is running in a Slurm job by checking for Slurm environment variables that specify how many CPUs are available and using that information to set the number of workers, if possible. Otherwise, the code checks for the number of cores on your computer and uses that value. To manually set the number of workers, change the first line by adding the value you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of workers to use for data loading\n",
    "num_workers = None  # To manually set the number of workers, change this to an integer\n",
    "\n",
    "if num_workers is None:\n",
    "    # If Slurm is being used, set the number of workers to a Slurm-provided value.\n",
    "    # If Slurm is not being used, set the number of workers to the number of available CPU cores.\n",
    "    if os.getenv(\"SLURM_CPUS_PER_TASK\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_CPUS_PER_TASK\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS_PER_NODE\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS_PER_NODE\"))\n",
    "    elif os.getenv(\"SLURM_NTASKS\") is not None:\n",
    "        num_workers = int(os.getenv(\"SLURM_NTASKS\"))\n",
    "    else:\n",
    "        num_workers = os.cpu_count()\n",
    "\n",
    "print(f\"Using {num_workers} workers for data loading.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the data augmentation in the `data_transforms` `train` section:\n",
    "\n",
    "In the code below, we are defining the transforms applied to images as they are loaded from disk and sent to the model. For the training data, data augmentation is a powerful method to increase the variability of the training data by randomly making changes to crop, orientation, color, etc. For more on data augmentation, see the *Practicum AI* Computer Vision course.\n",
    "\n",
    "Here is an example set of transformations. Where you see the `FIX_ME` in the code block below, add or adjust these as you think are appropriate for the data. \n",
    "\n",
    "```python\n",
    "[\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # Randomly flip images horizontally\n",
    "    transforms.RandomRotation(degrees=10),  # Randomly rotate images by up to 10 degrees\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # Randomly crop and resize\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color jitter\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Allow loading of truncated images, since the dataset's images aren't all the same size!\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set the batch size\n",
    "batch_size = 16\n",
    "\n",
    "# Define PyTorch data transforms\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "[\n",
    "            # FIX_ME: Add the transforms for the training data. \n",
    "            #        Include data augmentation as appropriate. \n",
    "            #        Note that the augmentation should not be applied to validation and test sets.\n",
    "]\n",
    "    ),\n",
    "    \"val\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "# Load PyTorch datasets\n",
    "image_datasets = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, data_transforms[\"train\"]),\n",
    "    \"val\": datasets.ImageFolder(val_dir, data_transforms[\"val\"]),\n",
    "    \"test\": datasets.ImageFolder(test_dir, data_transforms[\"test\"]),\n",
    "}\n",
    "\n",
    "# Create PyTorch data loaders\n",
    "dataloaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"train\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "    ),\n",
    "    \"val\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"val\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "    ),\n",
    "    \"test\": torch.utils.data.DataLoader(\n",
    "        image_datasets[\"test\"],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define the model\n",
    "\n",
    "\n",
    "Our first model will be taking the approach of adapting a model for our new task. [EfficientNet](https://en.wikipedia.org/wiki/EfficientNet) is a class of models released by Google Research in 2019 ([Tan and Le, 2019](https://arxiv.org/abs/1905.11946)). Figure 1 from the Tan and Le (2019) paper is below, showing classification accuracy vs model size. The red line connecting them highlights the good performance of EfficientNet models with relatively few parameters, especially compared to the competing models of the time.\n",
    "\n",
    "![Figure 1 from Tan and Le, 2019](images/EfficientNet_fig1.png)\n",
    "\n",
    "We will start using the EfficientNet-B5 model as a good tradeoff between accuracy and model size, however, feel free to try different models. The model is imported in the imports cell above. See the [PyTorch documentation](https://pytorch.org/vision/main/models.html) on available models and pre-trained weights.\n",
    "\n",
    "\n",
    "ðŸ“ **Note:**\n",
    "If you'd like more information on how CNNs work, we explored them as part of Deep Learning Foundations (DLF) course, and we have a full Computer Vision Intermediate course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the number of classes in your dataset\n",
    "num_classes = len(image_datasets[\"train\"].classes)\n",
    "\n",
    "# Load the EfficientNet-B5 model without pre-trained weights\n",
    "effNet_random_wt = efficientnet_b5(weights=None)\n",
    "\n",
    "# Replace the classifier (final layer) to match the number of classes\n",
    "effNet_random_wt.classifier[1] = nn.Linear(\n",
    "    effNet_random_wt.classifier[1].in_features, num_classes\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "effNet_random_wt.to(device)\n",
    "\n",
    "# Print the model architecture (optional)\n",
    "# You can right-click and select \"Clear Cell Outputs\" to clear the long model if you want\n",
    "print(effNet_random_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(effNet_random_wt.parameters(), lr=0.001)\n",
    "\n",
    "# How many epochs to run\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Set up the Pytorch Lightning training module\n",
    "\n",
    "The `LightningModule` is used to organize all the training information into structured sections. Briefly, the following sections control:\n",
    "\n",
    "* `__init__`: Initializes the model and sets up the data.\n",
    "* `forward`: Manages the forward pass through the model.\n",
    "* `training_step`:  Defines the training logic for a single batch.\n",
    "* `validation_step`: Defines the validation logic for a single batch.\n",
    "* `configure_optimizers`: Configures the optimizer(s) for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_Lightning(pl.LightningModule):\n",
    "    def __init__(self, model, criterion, optimizer):\n",
    "        super(model_Lightning, self).__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_acc\": [],\n",
    "        }  # Custom history variable\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.history[\"train_loss\"].append(loss.item())  \n",
    "        self.history[\"train_acc\"].append(acc.item())  \n",
    "        self.log(\"train_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        self.log(\"val_loss\", loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the training for the random weights\n",
    "\n",
    "Now we are ready to train the model, using the EfficientNet-B5 architecture, starting with random weights.\n",
    "\n",
    "### **Note**: This can take a lot of time\n",
    "\n",
    "Depending on the CPU and GPU resources available to you, training this may take a fair bit of time (**upwards of 20 minutes in many cases**). While a powerful GPU helps, having access to 12+ cores can also speed things up. This notebook uses parallel data loaders to feed images to the model as quickly as possible. As the number of available processors increases, the processors can keep the GPU fed with input images. With fewer than 8 cores, the GPU sits idle for much of the time, waiting for data from the processors. The speed of your filesystem is also a component here, but not as easy to change.\n",
    "\n",
    "**If you do not have the resources, or patience ðŸ˜‰, to train the model, skip the next cells, except the one in step 9.** We provide code in section 12 to download and import the results of a full training run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['SLURM_NTASKS_PER_NODE'] = '8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "logger = pl.loggers.TensorBoardLogger('agrinet_logs', name = 'EffNetB5-random-wts')\n",
    "\n",
    "# The trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs = epochs,\n",
    "    logger = logger,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "trainer.fit(\n",
    "    model_Lightning(effNet_random_wt, criterion, optimizer),\n",
    "    dataloaders[# FIX_ME: What data loader should be here?],\n",
    "    dataloaders[\"val\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the trained model\n",
    "\n",
    "Finally, we'll save the trained model to a file. We'll use the `torch.save()` method to save the model to a file, and the `torch.load()` method to load the model from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder to save the models if it does not exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Save the trained CNN model\n",
    "torch.save(effNet_random_wt.state_dict(), \"models/effNet_random_wt.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load the EfficientNet-B5 model wih pre-trained weights\n",
    "\n",
    "Next we'll train the EfficientNet-B5 model, starting with the weights from training the model on ImageNet. The `weights=DEFAULT` uses the latest training weights, which as of this writing is ImageNet v.2. Using this will get the current state of the art for a model, but you should check to see what that is when selecting that option.\n",
    "\n",
    "**Be sure to run this cell even if you don't have the resources to do the training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the EfficientNet model, this time keeping the pre-trained weights\n",
    "effNet_pretrain_wt = efficientnet_b5(weights=\"DEFAULT\")\n",
    "# Freeze all layers except the classifier\n",
    "for param in effNet_pretrain_wt.parameters():\n",
    "    param.requires_grad = False\n",
    "# Replace the classifier (final layer) to match the number of classes\n",
    "effNet_pretrain_wt.classifier[1] = nn.Linear(\n",
    "    effNet_pretrain_wt.classifier[1].in_features, num_classes\n",
    ")\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "effNet_pretrain_wt.to(device)\n",
    "\n",
    "# Define criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(effNet_pretrain_wt.parameters(), lr=0.001)\n",
    "\n",
    "# Print the model summary (optional; uncomment the next line to print the model)\n",
    "# print(effNet_pretrain_wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Fine-tune the EfficientNet-B5 model\n",
    "\n",
    "Next we'll train the EfficientNet-B5 model, starting with the weights from training the model on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up the logger\n",
    "logger = pl.loggers.TensorBoardLogger('agrinet_logs', name = 'EffNetB5-pretrained-wts')\n",
    "\n",
    "# The trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs = epochs,\n",
    "    logger = logger,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "trainer.fit(\n",
    "    model_Lightning(#FIX_ME: what model should be here?, criterion, optimizer),\n",
    "    dataloaders[# FIX_ME: What data loader should be here?],\n",
    "    dataloaders[# FIX_ME: What data loader should be here?],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save the EfficientNet-B5 trained from pre-trained weights model\n",
    "\n",
    "Finally, we'll save the trained model to a file. We'll use the `torch.save()` method to save the model to a file, and the `torch.load()` method to load the model from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a folder to save the models if it does not exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "# Save the fine-tuned EfficientNet-B5 model\n",
    "torch.save(effNet_pretrain_wt.state_dict(), \"models/effNet_pretrain_wt.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Explore the results of each training run\n",
    "\n",
    "From the information displayed during training, it should already be clear that the model starting from pre-trained weights did much better! Typical accuracies starting from scratch are around 40%, while when starting with pre-trained weights, they are closer to 80%!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### If you did not run the training\n",
    "> If you did not run the training, you can load a saved checkpoint using the code below.\n",
    ">\n",
    "> Uncomment the lines by **selecting all of the lines in the block** and using the keyboard shortcut **Ctrl-/**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Only uncomment this block if you did not train the models and need to \n",
    "# # download checkpoints.\n",
    "\n",
    "# # Download and extract the model training checkpoints\n",
    "# download_url = \"https://data.rc.ufl.edu/pub/practicum-ai/Transfer_Learning\"\n",
    "# file_name = \"agrinet_checkpoints.tar.gz\"\n",
    "# checkpoint_path = \"agrinet_logs\"\n",
    "# folder_names = [\"EffNetB5-pretrained-wts\", \"EffNetB5-random-wts\"]\n",
    "\n",
    "# helpers.download_and_extract_data(download_url, file_name, checkpoint_path, folder_names)\n",
    "\n",
    "# # Load the trained model starting from random weights\n",
    "# effNet_random_wt = model_Lightning.load_from_checkpoint('agrinet_logs/agrinet_logs/EffNetB5-random-wts/version_0/checkpoints/epoch=9-step=2250.ckpt',\n",
    "# \t\t\t\t\t\t\t\t\t\t\tmodel=effNet_random_wt,\n",
    "# \t\t\t\t\t\t\t\t\t\t\tcriterion=criterion,\n",
    "# \t\t\t\t\t\t\t\t\t\t\toptimizer=optimizer)\n",
    "\n",
    "# # Load the trained model starting from pre-trained weights\n",
    "# effNet_pretrain_wt = model_Lightning.load_from_checkpoint('agrinet_logs/agrinet_logs/EffNetB5-pretrained-wts/version_0/checkpoints/epoch=9-step=2250.ckpt',\n",
    "# \t\t\t\t\t\t\t\t\t\t\tmodel=effNet_pretrain_wt,\n",
    "# \t\t\t\t\t\t\t\t\t\t\tcriterion=criterion,\n",
    "# \t\t\t\t\t\t\t\t\t\t\toptimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for the random weights EfficientNet-B5 model\n",
    "\n",
    "# Ensure model is on the GPU\n",
    "effNet_random_wt.to(device)\n",
    "\n",
    "helpers.plot_confusion_matrix(effNet_random_wt, dataloaders[\"test\"], class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for the fine-tuned EfficientNet-B5 model\n",
    "\n",
    "# Ensure model is on the GPU\n",
    "effNet_pretrain_wt.to(device)\n",
    "\n",
    "helpers.plot_confusion_matrix(effNet_pretrain_wt, dataloaders[\"test\"], class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the confusion matrix plot\n",
    "plt.savefig(\"confusion_matrix.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tl_env",
   "language": "python",
   "name": "tl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
